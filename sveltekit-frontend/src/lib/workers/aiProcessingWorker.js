/**
 * AI Processing Worker
 * Handles AI tasks in separate threads for better performance
 */

let workerId = '';
let workerType = '';
let config = {};
let isInitialized = false;

// Worker state
let currentTask = null;
let tasksCompleted = 0;
let totalProcessingTime = 0;

// Message handler
self.onmessage = async function(event) {
	const { type, data } = event.data;

	try {
		switch (type) {
			case 'INIT':
				await handleInit(data);
				break;
			case 'PROCESS_TASK':
				await handleProcessTask(data.task);
				break;
			case 'STATUS_REQUEST':
				sendStatus();
				break;
			case 'TERMINATE':
				handleTerminate();
				break;
			default:
				console.warn(`Unknown message type: ${type}`);
		}
	} catch (error) {
		self.postMessage({
			type: 'WORKER_ERROR',
			data: {
				error: error.message,
				stack: error.stack,
				taskId: currentTask?.id
			}
		});
	}
};

// Initialize worker
async function handleInit(initData) {
	workerId = initData.workerId;
	workerType = initData.workerType;
	config = initData.config;
	isInitialized = true;

	console.log(`🧵 AI Worker ${workerId} (${workerType}) initialized`);

	// Send ready signal
	self.postMessage({
		type: 'WORKER_READY',
		data: { workerId, workerType }
	});
}

// Process AI task
async function handleProcessTask(task) {
	if (!isInitialized) {
		throw new Error('Worker not initialized');
	}

	currentTask = task;
	const startTime = Date.now();

	try {
		console.log(`🔄 Worker ${workerId} processing task ${task.id} (${task.type})`);

		let result;
		switch (task.type) {
			case 'embedding':
				result = await processEmbedding(task);
				break;
			case 'generation':
				result = await processGeneration(task);
				break;
			case 'analysis':
				result = await processAnalysis(task);
				break;
			case 'synthesis':
				result = await processSynthesis(task);
				break;
			case 'vector-search':
				result = await processVectorSearch(task);
				break;
			default:
				throw new Error(`Unsupported task type: ${task.type}`);
		}

		const duration = Date.now() - startTime;
		tasksCompleted++;
		totalProcessingTime += duration;

		// Send success result
		self.postMessage({
			type: 'TASK_COMPLETE',
			data: {
				taskId: task.id,
				success: true,
				result,
				duration,
				metrics: {
					tokensProcessed: result.tokensProcessed || 0,
					throughput: result.tokensProcessed ? (result.tokensProcessed / duration * 1000) : 0,
					memoryUsed: getMemoryUsage()
				}
			}
		});

		console.log(`✅ Worker ${workerId} completed task ${task.id} in ${duration}ms`);

	} catch (error) {
		const duration = Date.now() - startTime;
		
		// Send error result
		self.postMessage({
			type: 'TASK_ERROR',
			data: {
				taskId: task.id,
				success: false,
				error: error.message,
				duration
			}
		});

		console.error(`❌ Worker ${workerId} failed task ${task.id}:`, error);
	} finally {
		currentTask = null;
	}
}

// Task processors for different AI operations
async function processEmbedding(task) {
	const { provider, payload } = task;
	const { text, model = 'nomic-embed-text' } = payload;

	const response = await fetch(`${provider.endpoint}/api/embeddings`, {
		method: 'POST',
		headers: { 'Content-Type': 'application/json' },
		body: JSON.stringify({
			model,
			prompt: text
		})
	});

	if (!response.ok) {
		throw new Error(`Embedding API request failed: ${response.statusText}`);
	}

	const data = await response.json();
	return {
		embedding: data.embedding,
		tokensProcessed: text.split(' ').length // Approximate
	};
}

async function processGeneration(task) {
	const { provider, payload } = task;
	const { prompt, model = 'gemma3-legal', options = {} } = payload;

	const response = await fetch(`${provider.endpoint}/api/generate`, {
		method: 'POST',
		headers: { 'Content-Type': 'application/json' },
		body: JSON.stringify({
			model,
			prompt,
			stream: false,
			...options
		})
	});

	if (!response.ok) {
		throw new Error(`Generation API request failed: ${response.statusText}`);
	}

	const data = await response.json();
	return {
		text: data.response,
		tokensProcessed: estimateTokens(data.response),
		model: data.model,
		context: data.context
	};
}

async function processAnalysis(task) {
	const { provider, payload } = task;
	const { content, analysisType, options = {} } = payload;

	// For AutoGen/CrewAI integration, this would call their respective APIs
	// For now, using Ollama with specialized prompts
	
	let prompt;
	switch (analysisType) {
		case 'legal-document':
			prompt = `Analyze this legal document and extract key information:\n\nDocument:\n${content}\n\nProvide analysis in JSON format with: entities, key_points, legal_implications, and recommendations.`;
			break;
		case 'sentiment':
			prompt = `Analyze the sentiment of this text and provide a detailed breakdown:\n\n${content}\n\nProvide sentiment analysis in JSON format.`;
			break;
		case 'entity-extraction':
			prompt = `Extract all named entities from this text:\n\n${content}\n\nProvide entities in JSON format with types and confidence scores.`;
			break;
		default:
			prompt = `Analyze this content:\n\n${content}`;
	}

	const response = await fetch(`${provider.endpoint}/api/generate`, {
		method: 'POST',
		headers: { 'Content-Type': 'application/json' },
		body: JSON.stringify({
			model: 'gemma3-legal',
			prompt,
			stream: false,
			format: 'json',
			...options
		})
	});

	if (!response.ok) {
		throw new Error(`Analysis API request failed: ${response.statusText}`);
	}

	const data = await response.json();
	
	try {
		const analysis = JSON.parse(data.response);
		return {
			analysis,
			analysisType,
			tokensProcessed: estimateTokens(content + data.response)
		};
	} catch {
		// Fallback if JSON parsing fails
		return {
			analysis: { raw_response: data.response },
			analysisType,
			tokensProcessed: estimateTokens(content + data.response)
		};
	}
}

async function processSynthesis(task) {
	const { provider, payload } = task;
	const { sources, synthesisType, requirements = {} } = payload;

	const sourcesText = sources.map((source, index) => 
		`Source ${index + 1}:\n${source.content}\n`
	).join('\n');

	const prompt = `Synthesize the following sources into a coherent analysis:

${sourcesText}

Requirements:
- Type: ${synthesisType}
- Focus: ${requirements.focus || 'comprehensive analysis'}
- Length: ${requirements.length || 'medium'}
- Format: ${requirements.format || 'structured summary'}

Provide a well-structured synthesis that combines insights from all sources.`;

	const response = await fetch(`${provider.endpoint}/api/generate`, {
		method: 'POST',
		headers: { 'Content-Type': 'application/json' },
		body: JSON.stringify({
			model: 'gemma3-legal',
			prompt,
			stream: false
		})
	});

	if (!response.ok) {
		throw new Error(`Synthesis API request failed: ${response.statusText}`);
	}

	const data = await response.json();
	return {
		synthesis: data.response,
		sourcesCount: sources.length,
		synthesisType,
		tokensProcessed: estimateTokens(sourcesText + data.response)
	};
}

async function processVectorSearch(task) {
	const { provider, payload } = task;
	const { query, collection, limit = 10, filters = {} } = payload;

	// First, get the query embedding
	const embeddingResponse = await fetch(`${provider.endpoint}/api/embeddings`, {
		method: 'POST',
		headers: { 'Content-Type': 'application/json' },
		body: JSON.stringify({
			model: 'nomic-embed-text',
			prompt: query
		})
	});

	if (!embeddingResponse.ok) {
		throw new Error(`Embedding API request failed: ${embeddingResponse.statusText}`);
	}

	const embeddingData = await embeddingResponse.json();
	const queryVector = embeddingData.embedding;

	// For now, simulate vector search - in practice, this would call your vector DB
	// This would integrate with your PostgreSQL pgvector or Qdrant setup
	const mockResults = {
		results: [
			{
				id: '1',
				content: 'Sample document content...',
				similarity: 0.95,
				metadata: { title: 'Legal Document 1', date: '2024-01-01' }
			}
		],
		query,
		executionTime: Date.now()
	};

	return {
		searchResults: mockResults,
		queryVector,
		tokensProcessed: estimateTokens(query)
	};
}

// Utility functions
function estimateTokens(text) {
	// Rough estimation: ~4 characters per token
	return Math.ceil(text.length / 4);
}

function getMemoryUsage() {
	// In a real worker, you might track memory usage more precisely
	return `${Math.round(Math.random() * 100)}MB`;
}

function sendStatus() {
	self.postMessage({
		type: 'WORKER_STATUS',
		data: {
			workerId,
			workerType,
			isInitialized,
			currentTask: currentTask?.id || null,
			tasksCompleted,
			averageTaskTime: tasksCompleted > 0 ? totalProcessingTime / tasksCompleted : 0,
			status: currentTask ? 'busy' : 'idle'
		}
	});
}

function handleTerminate() {
	console.log(`🔄 Worker ${workerId} terminating...`);
	self.close();
}

// Handle worker errors
self.onerror = function(error) {
	console.error(`❌ Worker ${workerId} error:`, error);
	self.postMessage({
		type: 'WORKER_ERROR',
		data: {
			error: error.message,
			filename: error.filename,
			lineno: error.lineno
		}
	});
};

// Initial status report
setTimeout(() => {
	if (isInitialized) {
		sendStatus();
	}
}, 1000);