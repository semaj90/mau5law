
// LangChain + Ollama Integration with CUDA Support
// Production-ready AI service for legal document processing

import { ChatOllama } from "@langchain/ollama";
import { OllamaEmbeddings } from "@langchain/ollama";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { PromptTemplate } from "@langchain/core/prompts";
import type { Document as LangChainDocument } from "@langchain/core/documents";
import { VectorStoreRetriever } from "@langchain/core/vectorstores";
import { BaseRetriever } from "@langchain/core/retrievers";
import { CallbackManagerForRetrieverRun } from "@langchain/core/callbacks/manager";
// Crypto import handled dynamically to prevent SSR issues

// ============================================================================
// CONFIGURATION & TYPES
// ============================================================================

export interface LangChainConfig {
  ollamaBaseUrl: string;
  model: string;
  embeddingModel: string;
  temperature: number;
  maxTokens: number;
  chunkSize: number;
  chunkOverlap: number;
  maxRetrieverResults: number;
  useCuda: boolean;
  vectorDimensions: number;
}

export interface ProcessingResult {
  documentId: string;
  chunksCreated: number;
  embeddings: number[][];
  processingTime: number;
  metadata: {
    totalTokens: number;
    avgChunkSize: number;
    model: string;
  };
}

export interface QueryResult {
  answer: string;
  sources: Array<{
    content: string;
    metadata: Record<string, any>;
    score: number;
  }>;
  confidence: number;
  processingTime: number;
}

// Default configuration optimized for legal AI
const DEFAULT_CONFIG: LangChainConfig = {
  ollamaBaseUrl: "http://localhost:11434",
  model: "gemma3-legal:latest",
  embeddingModel: "nomic-embed-text:latest",
  temperature: 0.3,
  maxTokens: 2048,
  chunkSize: 1000,
  chunkOverlap: 200,
  maxRetrieverResults: 10,
  useCuda: true,
  vectorDimensions: 384
};

// ============================================================================
// ENHANCED LANGCHAIN + OLLAMA SERVICE
// ============================================================================

export class LangChainOllamaService {
  private config: LangChainConfig;
  private chatModel!: ChatOllama;
  private embeddings!: OllamaEmbeddings;
  private textSplitter!: RecursiveCharacterTextSplitter;
  private vectorStore: MemoryVectorStore | null = null;
  private isInitialized: boolean = false;

  constructor(config: Partial<LangChainConfig> = {}) {
    this.config = { ...DEFAULT_CONFIG, ...config };
    this.initializeModels();
    this.initializeTextSplitter();
  }

  private initializeModels() {
    // Initialize Chat Model with CUDA optimization
    this.chatModel = new ChatOllama({
      baseUrl: this.config.ollamaBaseUrl,
      model: this.config.model,
      temperature: this.config.temperature,
      // Note: numCtx, useGpu, numGpu, numThread may not be available in current ChatOllama version
    });

    // Initialize Embeddings with optimized settings
    this.embeddings = new OllamaEmbeddings({
      baseUrl: this.config.ollamaBaseUrl,
      model: this.config.embeddingModel,
      // Note: numGpu may not be available in current OllamaEmbeddings version
    });

    console.log('✅ LangChain + Ollama models initialized');
  }

  private initializeTextSplitter() {
    this.textSplitter = new RecursiveCharacterTextSplitter({
      chunkSize: this.config.chunkSize,
      chunkOverlap: this.config.chunkOverlap,
      separators: ["\n\n", "\n", ".", "!", "?", ",", " ", ""],
    });
  }

  // ========================================================================
  // DOCUMENT PROCESSING & EMBEDDING
  // ========================================================================

  async processDocument(
    content: string,
    metadata: Record<string, any> = {}
  ): Promise<ProcessingResult> {
    const startTime = Date.now();
    // Generate documentId without crypto to avoid SSR issues
    const documentId = `doc-${Date.now()}-${Math.random().toString(36).substring(2, 15)}`;

    try {
      // Split document into chunks
      const chunks = await this.textSplitter.splitText(content);
      
      // Create LangChain documents
      const documents = chunks.map((chunk, index) => ({
        pageContent: chunk,
        metadata: {
          ...metadata,
          documentId,
          chunkIndex: index,
          chunkId: `${documentId}_${index}`
        }
      }));

      // Create vector store if it doesn't exist
      if (!this.vectorStore) {
        this.vectorStore = await MemoryVectorStore.fromDocuments(
          documents,
          this.embeddings
        );
      } else {
        // Add documents to existing vector store
        await this.vectorStore.addDocuments(documents);
      }

      // Calculate embeddings for return data
      const embeddings = await Promise.all(
        chunks.map(chunk => this.embeddings.embedQuery(chunk))
      );

      const processingTime = Date.now() - startTime;
      const avgChunkSize = chunks.reduce((sum, chunk) => sum + chunk.length, 0) / chunks.length;

      const result: ProcessingResult = {
        documentId,
        chunksCreated: chunks.length,
        embeddings,
        processingTime,
        metadata: {
          totalTokens: content.length / 4, // Rough estimate
          avgChunkSize: Math.round(avgChunkSize),
          model: this.config.embeddingModel
        }
      };

      console.log(`✅ Processed document: ${chunks.length} chunks in ${processingTime}ms`);
      return result;

    } catch (error: any) {
      console.error('Document processing failed:', error);
      throw new Error(`Document processing failed: ${error.message}`);
    }
  }

  // ========================================================================
  // ENHANCED RAG QUERY
  // ========================================================================

  async queryDocuments(
    question: string,
    context: {
      documentTypes?: string[];
      dateRange?: { start: Date; end: Date };
      relevanceThreshold?: number;
      maxResults?: number;
    } = {}
  ): Promise<QueryResult> {
    if (!this.vectorStore) {
      throw new Error('No documents have been processed yet. Call processDocument first.');
    }

    const startTime = Date.now();
    const maxResults = context.maxResults || this.config.maxRetrieverResults;
    const relevanceThreshold = context.relevanceThreshold || 0.7;

    try {
      // Create retriever with enhanced filtering
      const retriever = this.vectorStore.asRetriever({
        k: maxResults,
        searchType: "similarity",
        // Note: searchKwargs may not be available in current version
        filter: (doc) => true // Simple filter function
      });

      // Get relevant documents
      const relevantDocs = await retriever.getRelevantDocuments(question);

      // Filter documents based on context if provided
      const filteredDocs = this.filterDocumentsByContext(relevantDocs, context);

      // Create enhanced prompt for legal AI
      const prompt = this.createLegalPrompt(question, filteredDocs);

      // Generate response using chat model
      const response = await this.chatModel.invoke(prompt);

      // Calculate confidence based on document relevance
      const confidence = this.calculateConfidence(filteredDocs, question);

      const processingTime = Date.now() - startTime;

      const result: QueryResult = {
        answer: response.content as string,
        sources: filteredDocs.map(doc => ({
          content: doc.pageContent,
          metadata: doc.metadata,
          score: doc.metadata.score || 0.8
        })),
        confidence,
        processingTime
      };

      console.log(`✅ Query processed in ${processingTime}ms with ${filteredDocs.length} sources`);
      return result;

    } catch (error: any) {
      console.error('Query processing failed:', error);
      throw new Error(`Query processing failed: ${error.message}`);
    }
  }

  // ========================================================================
  // HELPER METHODS
  // ========================================================================

  private filterDocumentsByContext(
    documents: LangChainDocument[],
    context: any
  ): LangChainDocument[] {
    let filtered = documents;

    // Filter by document types
    if (context.documentTypes && context.documentTypes.length > 0) {
      filtered = filtered.filter(doc => 
        context.documentTypes.includes(doc.metadata.type)
      );
    }

    // Filter by date range
    if (context.dateRange) {
      filtered = filtered.filter(doc => {
        const docDate = new Date(doc.metadata.createdAt);
        return docDate >= context.dateRange.start && docDate <= context.dateRange.end;
      });
    }

    return filtered;
  }

  private createLegalPrompt(question: string, documents: LangChainDocument[]): string {
    const context = documents
      .map(doc => `[Source: ${doc.metadata.chunkId}]\n${doc.pageContent}`)
      .join('\n\n');

    return `You are a legal AI assistant specializing in document analysis and legal research. 
Use the provided context to answer the question accurately and professionally.

Context:
${context}

Question: ${question}

Instructions:
- Provide a comprehensive, accurate answer based on the context
- Cite specific sources using [Source: ID] format
- If the context doesn't contain sufficient information, state this clearly
- Use legal terminology appropriately
- Structure your response clearly with bullet points or numbered lists when appropriate

Answer:`;
  }

  private calculateConfidence(documents: LangChainDocument[], question: string): number {
    if (documents.length === 0) return 0.1;
    
    // Simple confidence calculation based on document count and relevance
    const avgScore = documents.reduce((sum, doc) => sum + (doc.metadata.score || 0.8), 0) / documents.length;
    const documentCountFactor = Math.min(documents.length / 5, 1.0);
    const questionLengthFactor = Math.min(question.length / 50, 1.0);
    
    return Math.min(avgScore * documentCountFactor * questionLengthFactor, 0.95);
  }

  // ========================================================================
  // UTILITY METHODS
  // ========================================================================

  async testConnection(): Promise<boolean> {
    try {
      const testResponse = await this.chatModel.invoke("Hello, this is a connection test.");
      this.isInitialized = true;
      return !!testResponse;
    } catch (error: any) {
      console.error('Connection test failed:', error);
      return false;
    }
  }

  getStats() {
    return {
      config: this.config,
      isInitialized: this.isInitialized,
      vectorStoreSize: this.vectorStore?.memoryVectors?.length || 0,
      model: this.config.model,
      embeddingModel: this.config.embeddingModel
    };
  }

  // Clear vector store and reset
  reset() {
    this.vectorStore = null;
    this.isInitialized = false;
    console.log('🔄 LangChain service reset');
  }
}

// Export singleton instance for global use
export const langChainOllamaService = new LangChainOllamaService();
;
// Note: Types are already exported as interfaces above